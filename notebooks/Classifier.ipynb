{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-04T09:33:48.469043Z",
     "start_time": "2025-03-04T09:33:31.714617Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from data.pyg_dataToGraph import DataToGraph\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:23.005707Z",
     "start_time": "2025-03-04T09:34:22.828442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO 加载数据集\n",
    "dataset = DataToGraph(\n",
    "    raw_data_path='../../data/',\n",
    "    dataset_name='TFF' + '.mat')  # 格式: [(graph,label),...,(graph,label)]\n",
    "\n",
    "input_dim = dataset[0].x.size(1)\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# 提取所有的x和y\n",
    "x0 = []\n",
    "labels = []\n",
    "\n",
    "for data in dataset:\n",
    "    # 提取x (形状为 [num_nodes, input_dim])\n",
    "    # 但是你提到dataset.x的形状是 [24,50]，这可能是一个图的x特征矩阵\n",
    "    x0.append(data.x)\n",
    "    # 提取y（标量标签）\n",
    "    labels.append(data.y)\n",
    "\n",
    "# 将列表转换为张量\n",
    "x0 = torch.stack(x0)  # 形状 [num_samples, 24, 50]\n",
    "labels = torch.stack(labels)  # 形状 [num_samples]\n",
    "\n",
    "print(num_classes)\n",
    "print(\"X0 shape:\", x0.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ],
   "id": "f6e1a3fc4eb53ea2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "X0 shape: torch.Size([2368, 24, 50])\n",
      "Labels shape: torch.Size([2368, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:23.728702Z",
     "start_time": "2025-03-04T09:34:23.722554Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")",
   "id": "6a52cbd962af9c9a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:23.807744Z",
     "start_time": "2025-03-04T09:34:23.784320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将数据传输到GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TODO 确定超参数的值\n",
    "# 超参数值\n",
    "num_steps = 1000  # 假设扩散步数为 1000\n",
    "eps = 1e-5  # 避免除以零或引入过小的数值的小偏移量\n",
    "\n",
    "# 生成时间步的序列\n",
    "t = torch.linspace(0, 1, num_steps + 1)  # 主要时间步范围从 0 到 1\n",
    "\n",
    "# 使用余弦调度生成 betas\n",
    "betas = torch.cos(torch.pi / 2.0 * t) ** 2  # 余弦平方函数\n",
    "betas = betas / betas.max()  # 归一化到 0-1 范围\n",
    "betas = torch.flip(betas, [0])  # 反转顺序，以确保从小到大递增\n",
    "betas = torch.clamp(betas, min=1e-5, max=0.5e-2)  # 调整范围到 (1e-5, 0.5e-2)\n",
    "\n",
    "# 计算 alpha , alpha_prod , alpha_prod_previous , alpha_bar_sqrt 等变量的值\n",
    "alphas = 1 - betas\n",
    "alphas_prod = torch.cumprod(alphas, dim=0)  # 累积连乘\n",
    "alphas_prod_p = torch.cat([torch.tensor([1]).float(), alphas_prod[:-1]], 0)  # p means previous\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_prod)\n",
    "one_minus_alphas_bar_log = torch.log(1 - alphas_prod)\n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)\n",
    "\n",
    "# 将超参数也移动到GPU\n",
    "betas = betas.to(device)\n",
    "alphas = alphas.to(device)\n",
    "alphas_prod = alphas_prod.to(device)\n",
    "alphas_prod_p = alphas_prod_p.to(device)\n",
    "alphas_bar_sqrt = alphas_bar_sqrt.to(device)\n",
    "one_minus_alphas_bar_log = one_minus_alphas_bar_log.to(device)\n",
    "one_minus_alphas_bar_sqrt = one_minus_alphas_bar_sqrt.to(device)\n",
    "\n",
    "assert alphas_prod.shape == alphas_prod.shape == alphas_prod_p.shape \\\n",
    "       == alphas_bar_sqrt.shape == one_minus_alphas_bar_log.shape \\\n",
    "       == one_minus_alphas_bar_sqrt.shape\n",
    "print(\"all the same shape:\", betas.shape)"
   ],
   "id": "1a7ea83357d98794",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the same shape: torch.Size([1001])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:23.843671Z",
     "start_time": "2025-03-04T09:34:23.839467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def q_sample(x0, t, noise):\n",
    "    \"\"\"前向扩散过程：根据时间步t给x0加噪\"\"\"\n",
    "    sqrt_alpha_prod = torch.sqrt(alphas_prod[t]).view(-1, 1, 1)\n",
    "    sqrt_one_minus_alpha_prod = torch.sqrt(1 - alphas_prod[t]).view(-1, 1, 1)\n",
    "    return sqrt_alpha_prod * x0 + sqrt_one_minus_alpha_prod * noise"
   ],
   "id": "2339b8d5d325cdc4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MySequential",
   "id": "eb8c19daf22706b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:23.879049Z",
     "start_time": "2025-03-04T09:34:23.875614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MySequential(nn.Sequential):\n",
    "    def forward(self, x, t_emb):\n",
    "        for module in self:\n",
    "            if isinstance(module, ConditionalBlock):  # 仅对特定模块传参\n",
    "                x = module(x, t_emb)\n",
    "            else:  # 其他模块按默认方式处理\n",
    "                x = module(x)\n",
    "        return x"
   ],
   "id": "6afdb1786819d31c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conditional Embedding",
   "id": "7cfc68705c4612c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:23.913107Z",
     "start_time": "2025-03-04T09:34:23.906824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "def sinusoidal_embedding(t, dim):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        t: 时间步张量 [batch_size, ]\n",
    "        dim: 嵌入维度\n",
    "    Returns:\n",
    "        嵌入向量 [batch_size, dim]\n",
    "    \"\"\"\n",
    "    device = t.device\n",
    "    half_dim = dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "    emb = t.float()[:, None] * emb[None, :]  # [batch_size, half_dim]\n",
    "\n",
    "    # 拼接正弦和余弦分量\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "\n",
    "    # 处理奇数维度情况\n",
    "    if dim % 2 == 1:\n",
    "        emb = F.pad(emb, (0, 1), mode='constant')\n",
    "\n",
    "    return emb\n",
    "\n",
    "\n",
    "class ConditionalEmbedding(nn.Module):\n",
    "    def __init__(self, num_classes, time_dim=256, label_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim)\n",
    "        )\n",
    "        self.label_embed = nn.Embedding(num_classes, label_dim)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(time_dim + label_dim, time_dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 2, time_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        # t: [B,] 时间步\n",
    "        # y: [B,] 标签\n",
    "        t_emb = sinusoidal_embedding(t, self.time_embed[0].in_features)\n",
    "        t_emb = self.time_embed(t_emb)  # [B, time_dim]\n",
    "\n",
    "        l_emb = self.label_embed(y).squeeze(1)  # [B, label_dim]\n",
    "\n",
    "        # 融合时间与标签信息\n",
    "        combined = torch.cat([t_emb, l_emb], dim=1)\n",
    "        return self.fusion(combined)  # [B, time_dim]"
   ],
   "id": "ef33d22a52c59b27",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:23.925953Z",
     "start_time": "2025-03-04T09:34:23.921113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EnhancedTimeEmbedding(nn.Module):\n",
    "    \"\"\"增强时间嵌入（添加多层感知）\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.GELU(),\n",
    "            #nn.SiLU(),\n",
    "            nn.Linear(dim*4, dim),\n",
    "            #nn.SiLU(),\n",
    "            #nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return self.embed(emb)"
   ],
   "id": "46439c1ffa7511a9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conditional Block",
   "id": "f0196aceaeb0ed22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:23.949863Z",
     "start_time": "2025-03-04T09:34:23.936835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from einops import rearrange\n",
    "\n",
    "class ConditionalBlock(nn.Module):\n",
    "    \"\"\"基于你原有MyBlock改造的条件版本\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, cond_dim, mult=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cond_dim: 条件向量的维度 (time+label的融合维度)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 修改后的条件投影层（移除偏置项）验证条件注入的有效性\n",
    "        self.cond_mlp = nn.Sequential(\n",
    "            nn.Linear(cond_dim, out_ch*2, bias=False),  # 关键修改：bias=False\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        # 保持原有卷积结构\n",
    "        self.ds_conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.GroupNorm(1, out_ch),\n",
    "            nn.Conv2d(out_ch, out_ch * mult, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.GroupNorm(1, out_ch * mult),\n",
    "            nn.Conv2d(out_ch * mult, out_ch, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond_emb):\n",
    "        \"\"\"输入变化：t_emb → cond_emb (融合时间+标签的条件向量)\"\"\"\n",
    "        h = self.ds_conv(x)\n",
    "\n",
    "        # 条件注入 (scale and shift)\n",
    "        scale, shift = self.cond_mlp(cond_emb).chunk(2, dim=1)  # [B, 2*out_ch] → [B, out_ch], [B, out_ch]\n",
    "        h = h * (1 + scale[:, :, None, None])  # 缩放\n",
    "        h = h + shift[:, :, None, None]        # 偏移\n",
    "\n",
    "        h = self.conv(h)\n",
    "        return h + self.res_conv(x)  # 保持原有残差连接"
   ],
   "id": "13490e2ab9eb36ea",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Residual",
   "id": "9375f275ff35bf03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:23.965141Z",
     "start_time": "2025-03-04T09:34:23.960142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 残差模块，将输入加到输出上\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x"
   ],
   "id": "edd6143460bc48f3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention",
   "id": "27a8e4973ba08f6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:23.982655Z",
     "start_time": "2025-03-04T09:34:23.975843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Attention\n",
    "from torch import einsum, softmax\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b\"\n",
    "                     \" h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)"
   ],
   "id": "ae17307e2f1ce159",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PreNorm",
   "id": "7a2a7cda29003ea7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:24.008105Z",
     "start_time": "2025-03-04T09:34:24.003291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ],
   "id": "442d84136cf723b2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### NoisyDataClassifier",
   "id": "9c58bde2bdf3ebe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:24.022111Z",
     "start_time": "2025-03-04T09:34:24.016110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "class UNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, time_dim=128):\n",
    "        super().__init__()\n",
    "        chs = [1, 64, 128, 256]  # 输入通道调整为1 (单通道特征图)\n",
    "\n",
    "        # 时间嵌入层 (移除标签条件)\n",
    "        self.time_embed = EnhancedTimeEmbedding(time_dim)\n",
    "\n",
    "        # 下采样路径 (增强特征提取)\n",
    "        self.down = nn.ModuleList([\n",
    "            MySequential(\n",
    "                ConditionalBlock(chs[i], chs[i+1], cond_dim=time_dim),\n",
    "                ConditionalBlock(chs[i+1], chs[i+1], cond_dim=time_dim),\n",
    "                Residual(PreNorm(chs[i+1], LinearAttention(chs[i+1])))\n",
    "            ) for i in range(len(chs)-1)\n",
    "        ])\n",
    "\n",
    "        # 中间层 (适配分类任务)\n",
    "        self.mid = MySequential(\n",
    "            ConditionalBlock(chs[-1], chs[-1]*2, cond_dim=time_dim),\n",
    "            Residual(PreNorm(chs[-1]*2, Attention(chs[-1]*2))),\n",
    "            ConditionalBlock(chs[-1]*2, chs[-1], cond_dim=time_dim)\n",
    "        )\n",
    "\n",
    "        # 分类头\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),  # 全局平均池化\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(chs[-1], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        输入:\n",
    "            x: [B, H, W] (如 [B,24,50])\n",
    "            t: [B,] 时间步\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)  # [B,1,24,50]\n",
    "\n",
    "        # 时间嵌入\n",
    "        t_emb = self.time_embed(t)\n",
    "\n",
    "        skips = []\n",
    "\n",
    "        # 编码器\n",
    "        for block in self.down:\n",
    "            x = block(x, t_emb)\n",
    "            skips.append(x)\n",
    "            x = F.max_pool2d(x, kernel_size=(2,1))  # 保持宽度不变\n",
    "\n",
    "        # 中间处理\n",
    "        x = self.mid(x, t_emb)\n",
    "\n",
    "        # 分类\n",
    "        return self.classifier(x)\n"
   ],
   "id": "6867136b77fada9c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###",
   "id": "1ce081f9fcfc91d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### train_Classifier",
   "id": "325ee603e8195520"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:24.036970Z",
     "start_time": "2025-03-04T09:34:24.032499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设相关自定义模块已定义（ConditionalBlock, LinearAttention等）\n",
    "# 若实际模块定义不同，需调整此处导入\n",
    "\n",
    "def test_unet_classifier():\n",
    "    # 配置参数\n",
    "    batch_size = 4\n",
    "    input_shape = (24, 50)  # 高 x 宽\n",
    "    num_classes = 7\n",
    "    time_dim = 128\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 固定随机种子\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # 初始化模型\n",
    "    model = UNetClassifier(num_classes=num_classes, time_dim=time_dim).to(device)\n",
    "\n",
    "    # 生成测试数据\n",
    "    x_test = torch.randn(batch_size, *input_shape).to(device)  # [4,24,50]\n",
    "    t_test = torch.randint(0, 1000, (batch_size,)).to(device)  # 随机时间步 [4,]\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_test, t_test)\n",
    "\n",
    "    # 验证输出维度\n",
    "    expected_shape = (batch_size, num_classes)\n",
    "    assert logits.shape == expected_shape, (\n",
    "        f\"输出形状错误！期望: {expected_shape}, 实际: {logits.shape}\"\n",
    "    )\n",
    "\n",
    "    # 打印测试结果\n",
    "    print(f\"输入形状: {x_test.shape}\")\n",
    "    print(f\"时间步形状: {t_test.shape}\")\n",
    "    print(f\"输出logits形状: {logits.shape}\")\n",
    "    print(\"测试通过！\")"
   ],
   "id": "eb70a7d0e3c2ce9e",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:24.332682Z",
     "start_time": "2025-03-04T09:34:24.050972Z"
    }
   },
   "cell_type": "code",
   "source": "test_unet_classifier()",
   "id": "94eba1bc1df4e44d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([4, 24, 50])\n",
      "时间步形状: torch.Size([4])\n",
      "输出logits形状: torch.Size([4, 7])\n",
      "测试通过！\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6cc86a49156676fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:34:24.350479Z",
     "start_time": "2025-03-04T09:34:24.342915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_conditional_classifier(\n",
    "    x0,                  # 原始数据 [N,24,50]\n",
    "    labels,              # 标签 [N]\n",
    "    num_classes,         # 类别数\n",
    "    num_steps=1000,      # 扩散总步数\n",
    "    batch_size=128,\n",
    "    lr=1e-4,\n",
    "    epochs=500,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_path='best_classifier.pth'\n",
    "):\n",
    "    # 数据预处理\n",
    "    dataset = TensorDataset(x0, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    # 初始化模型\n",
    "    model = UNetClassifier(num_classes=num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    # 训练记录\n",
    "    best_loss = float('inf')\n",
    "    history = {'train_loss': [], 'acc': []}\n",
    "\n",
    "    print(f\"\\n🚀 开始训练 | 设备: {device}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"样本数: {len(x0)} | 类别数: {num_classes}\")\n",
    "    print(f\"批次大小: {batch_size} | 初始学习率: {lr}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        for batch_x0, batch_y in pbar:\n",
    "            # 数据转移至设备\n",
    "            batch_x0 = batch_x0.to(device)        # [B,24,50]\n",
    "            batch_y = batch_y.to(device)          # [B]\n",
    "\n",
    "            # 生成随机时间步\n",
    "            b = batch_x0.size(0)\n",
    "            t = torch.randint(0, num_steps, (b,), device=device)\n",
    "\n",
    "            # 前向加噪\n",
    "            noise = torch.randn_like(batch_x0)\n",
    "            sqrt_alpha = torch.sqrt(alphas_prod[t]).view(-1,1,1)\n",
    "            sqrt_one_minus_alpha = torch.sqrt(1 - alphas_prod[t]).view(-1,1,1)\n",
    "            noisy_x = sqrt_alpha * batch_x0 + sqrt_one_minus_alpha * noise\n",
    "\n",
    "            # 模型前向\n",
    "            logits = model(noisy_x, t)\n",
    "\n",
    "            batch_y = batch_y.squeeze(1)\n",
    "            # 计算损失\n",
    "            loss = F.cross_entropy(logits, batch_y)\n",
    "\n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # 统计指标\n",
    "            epoch_loss += loss.item() * b\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total += b\n",
    "\n",
    "            # 更新进度条\n",
    "            pbar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'acc': f\"{correct/total:.2%}\"\n",
    "            })\n",
    "\n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "        # 计算epoch指标\n",
    "        epoch_loss /= len(dataset)\n",
    "        epoch_acc = correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"💾 保存最佳模型 | 损失: {best_loss:.4f}\")\n",
    "\n",
    "        # 打印epoch结果\n",
    "        print(f\"Epoch {epoch:03d} | \"\n",
    "              f\"Loss: {epoch_loss:.4f} | \"\n",
    "              f\"Acc: {epoch_acc:.2%} | \"\n",
    "              f\"LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    print(\"✅ 训练完成!\")\n",
    "    return model, history\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设已加载数据\n",
    "    # x0: [N,24,50], labels: [N]\n",
    "\n",
    "    model, history = train_conditional_classifier(\n",
    "        x0=x0,\n",
    "        labels=labels,\n",
    "        num_classes=num_classes,\n",
    "        num_steps=200,\n",
    "        batch_size=64,\n",
    "        epochs=200,\n",
    "        save_path='best_noisy_classifier.pth'\n",
    "    )'''\n"
   ],
   "id": "25ab5158e53825ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from torch.utils.data import DataLoader, TensorDataset\\nfrom tqdm import tqdm\\nimport torch.nn.functional as F\\n\\ndef train_conditional_classifier(\\n    x0,                  # 原始数据 [N,24,50]\\n    labels,              # 标签 [N]\\n    num_classes,         # 类别数\\n    num_steps=1000,      # 扩散总步数\\n    batch_size=128,\\n    lr=1e-4,\\n    epochs=500,\\n    device=\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\',\\n    save_path=\\'best_classifier.pth\\'\\n):\\n    # 数据预处理\\n    dataset = TensorDataset(x0, labels)\\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\\n\\n    # 初始化模型\\n    model = UNetClassifier(num_classes=num_classes).to(device)\\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\\n\\n    # 训练记录\\n    best_loss = float(\\'inf\\')\\n    history = {\\'train_loss\\': [], \\'acc\\': []}\\n\\n    print(f\"\\n🚀 开始训练 | 设备: {device}\")\\n    print(\"---------------------------------------\")\\n    print(f\"样本数: {len(x0)} | 类别数: {num_classes}\")\\n    print(f\"批次大小: {batch_size} | 初始学习率: {lr}\")\\n    print(\"---------------------------------------\")\\n\\n    for epoch in range(1, epochs+1):\\n        model.train()\\n        epoch_loss = 0.0\\n        correct = 0\\n        total = 0\\n\\n        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\\n        for batch_x0, batch_y in pbar:\\n            # 数据转移至设备\\n            batch_x0 = batch_x0.to(device)        # [B,24,50]\\n            batch_y = batch_y.to(device)          # [B]\\n\\n            # 生成随机时间步\\n            b = batch_x0.size(0)\\n            t = torch.randint(0, num_steps, (b,), device=device)\\n\\n            # 前向加噪\\n            noise = torch.randn_like(batch_x0)\\n            sqrt_alpha = torch.sqrt(alphas_prod[t]).view(-1,1,1)\\n            sqrt_one_minus_alpha = torch.sqrt(1 - alphas_prod[t]).view(-1,1,1)\\n            noisy_x = sqrt_alpha * batch_x0 + sqrt_one_minus_alpha * noise\\n\\n            # 模型前向\\n            logits = model(noisy_x, t)\\n\\n            batch_y = batch_y.squeeze(1)\\n            # 计算损失\\n            loss = F.cross_entropy(logits, batch_y)\\n\\n            # 反向传播\\n            optimizer.zero_grad()\\n            loss.backward()\\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n            optimizer.step()\\n\\n            # 统计指标\\n            epoch_loss += loss.item() * b\\n            _, predicted = torch.max(logits, 1)\\n            correct += (predicted == batch_y).sum().item()\\n            total += b\\n\\n            # 更新进度条\\n            pbar.set_postfix({\\n                \\'loss\\': loss.item(),\\n                \\'acc\\': f\"{correct/total:.2%}\"\\n            })\\n\\n        # 更新学习率\\n        scheduler.step()\\n\\n        # 计算epoch指标\\n        epoch_loss /= len(dataset)\\n        epoch_acc = correct / total\\n        history[\\'train_loss\\'].append(epoch_loss)\\n        history[\\'acc\\'].append(epoch_acc)\\n\\n        # 保存最佳模型\\n        if epoch_loss < best_loss:\\n            best_loss = epoch_loss\\n            torch.save(model.state_dict(), save_path)\\n            print(f\"💾 保存最佳模型 | 损失: {best_loss:.4f}\")\\n\\n        # 打印epoch结果\\n        print(f\"Epoch {epoch:03d} | \"\\n              f\"Loss: {epoch_loss:.4f} | \"\\n              f\"Acc: {epoch_acc:.2%} | \"\\n              f\"LR: {scheduler.get_last_lr()[0]:.2e}\")\\n\\n    print(\"✅ 训练完成!\")\\n    return model, history\\n\\n# 使用示例\\nif __name__ == \"__main__\":\\n    # 假设已加载数据\\n    # x0: [N,24,50], labels: [N]\\n\\n    model, history = train_conditional_classifier(\\n        x0=x0,\\n        labels=labels,\\n        num_classes=num_classes,\\n        num_steps=200,\\n        batch_size=64,\\n        epochs=200,\\n        save_path=\\'best_noisy_classifier.pth\\'\\n    )'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
